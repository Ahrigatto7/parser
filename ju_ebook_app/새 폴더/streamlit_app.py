import io
import re
from collections import Counter

import pandas as pd
import pdfplumber
import streamlit as st
from openai import OpenAI


def extract_text(uploaded_file) -> str:
    """Return plain text from various document types."""
    ext = uploaded_file.name.split(".")[-1].lower()
    if ext == "pdf":
        with pdfplumber.open(uploaded_file) as pdf:
            pages = [page.extract_text() or "" for page in pdf.pages]
        return "\n".join(pages)
    if ext in {"xlsx", "xls"}:
        df = pd.read_excel(uploaded_file)
        return df.to_csv(index=False)
    return uploaded_file.read().decode()


def analyze_structure(text: str) -> list[str]:
    """Extract simple heading-like structures from text."""
    headings = []
    for line in text.splitlines():
        if re.match(r"^\s*(\d+\.\s+|[#\*-])", line):
            headings.append(line.strip())
    return headings


def extract_rules(text: str) -> list[tuple[str, int]]:
    """Return the most common words in the text as simple rules."""
    tokens = re.findall(r"\b\w+\b", text.lower())
    return Counter(tokens).most_common(10)


# UI Layout
st.title("ğŸ“Š ë©”ì¸ ëŒ€ì‹œë³´ë“œ")
st.write(
    "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ, êµ¬ì¡° ë¶„ì„, ê·œì¹™ ì¶”ì¶œì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    " OpenAI API í‚¤ë¥¼ ì œê³µí•˜ë©´ GPTì—ê²Œ ì§ˆë¬¸ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)

openai_api_key = st.text_input("OpenAI API Key", type="password")

if not openai_api_key:
    st.info("Please add your OpenAI API key to continue.", icon="ğŸ—ï¸")
else:
    client = OpenAI(api_key=openai_api_key)
    uploaded_file = st.file_uploader(
        "ë¬¸ì„œ ì—…ë¡œë“œ", type=("txt", "md", "pdf", "xlsx", "xls")
    )

    text, headings, rules = "", [], []
    if uploaded_file:
        text = extract_text(uploaded_file)

        tabs = st.tabs([
            "í…ìŠ¤íŠ¸ ì¸ì‹",
            "êµ¬ì¡°í™” ë¶„ì„",
            "ê·œì¹™ ì¶”ì¶œ",
            "ê·œì¹™ ì €ì¥ & ë¦¬í¬íŠ¸",
        ])

        with tabs[0]:
            st.subheader("í…ìŠ¤íŠ¸ ì¸ì‹")
            st.text_area("ì¶”ì¶œëœ í…ìŠ¤íŠ¸", text, height=300)

        with tabs[1]:
            st.subheader("êµ¬ì¡°í™” ë¶„ì„")
            headings = analyze_structure(text)
            st.write(headings if headings else "ê°ì§€ëœ êµ¬ì¡°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        with tabs[2]:
            st.subheader("ê·œì¹™ ì¶”ì¶œ")
            rules = extract_rules(text)
            st.write(rules)

        with tabs[3]:
            st.subheader("ê·œì¹™ ì €ì¥ & ë¦¬í¬íŠ¸")
            report = f"Headings:\n{'\n'.join(headings)}\n\nRules:\n" + "\n".join(
                f"{word}: {count}" for word, count in rules
            )
            st.download_button(
                "ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", data=report, file_name="report.txt"
            )

    question = st.text_area(
        "ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!",
        placeholder="ì˜ˆ) ê°„ë‹¨íˆ ìš”ì•½í•´ ì¤˜",
        disabled=not text,
    )

    if text and question:
        messages = [{
            "role": "user",
            "content": f"Here's a document: {text} \n\n---\n\n {question}",
        }]
        stream = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            stream=True,
        )
        st.write_stream(stream)
